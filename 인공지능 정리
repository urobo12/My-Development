XOR문제 -> 다층 퍼셉트론으로 해결
은닉층에 포함된 가중치는 어떻게 업데이트 하나?
인공지능의 겨울 시작

"오차역전파"를 통해 해결


out(1-out)형태 => 델타식

델타식을 이용해 깊은 신경망의 계산이 가능해짐
  깊은 층을 만들면, 출력층에서 시작된 가중치 업데이트가 처음까지 전달 안되는 경우
  활성화 함수를 시그모이드로 사용하여 그 특성 때문에 발생


제프리 힌튼 교수가 렐루(ReLU) 새로운 활성화 함수 제안
  ReLU,하이퍼볼릭 탄젠트, 소프트플러스 등


정확도와 속도를 모두 향상 시킨 아담(adam)이라는 고급 경사하강법이 가장 많이 쓰임

    
오차를 최소화 하는 경사하강법을 딥러닝용어로 Optimizer
    텐서플로우의 optimizers 옵션으로 지정
    활성화함수(시그모이드, 렐루 등)은 activation 에 지정

실행횟수 - iterations, 학습율 - lr, 모멘텀 계수 - mo 설정
  



  



